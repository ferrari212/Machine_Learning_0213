{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "AI workshop â€“ from hype to real-world applications: Traffic sign classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferrari212/Machine_Learning_0213/blob/master/AI_workshop_%E2%80%93_from_hype_to_real_world_applications_Traffic_sign_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei84sHtRtqQb",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning for image classification\n",
        "For Workshop background and intro go to https://www.tekna.no/en/events/ai-workshop--from-hype-to-real-world-applications-39390/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwNqN9cztqQe",
        "colab_type": "text"
      },
      "source": [
        "The following tutorial covers how to set up a simple deep learning experiment for image classification. The approach is based on using building blocks and modules from the machine learning framework \"Tensorflow\". \n",
        "\n",
        "This model will classify images into different categories. The prerequisites for setting up the model is access to labelled data. As an initial test case we have included some example images of various traffic signs. The task of the model is thus to predict what kind of sign it sees. To make the example more realistic, both image quality and amount of data are quite limited (as is often the case in practical applications of machine learning). \n",
        "\n",
        "These images are of course only included as an example to get started, and could easily be replaced with other images as long as you follow the same folder structure as the current setup, as explained below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6rRyUB9uX3S",
        "colab_type": "text"
      },
      "source": [
        "## Helper code for loading data and setting tensorflow version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90s1hZ6RZWoN",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "# Use Tensorflow version 1.x\n",
        "# This is a colab specific command that can be used to set a specific version of Tensorflow.\n",
        "# Both Colab and Tensorflow are Google products, so expect some \"shortcuts\" and tricks that are Colab specific.\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpPTNgP1y0Qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Access training, validation and test data from a google drive account. Here\n",
        "# you will need to authenticate with your own google account.\n",
        "\n",
        "# File url: \"https://drive.google.com/open?id=1HQiZx5tugjmdK4wEHLGxgfaghBLAS6Mx\"\n",
        "GDRIVE_FILE_ID = \"1HQiZx5tugjmdK4wEHLGxgfaghBLAS6Mx\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdJ7mrgDuXPd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "# This cell contains code needed to import data from Google Drive.\n",
        "# You don't need to understand this code!\n",
        "\n",
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "def gdrive_zip_to_working_dir(file_id, filename='data.zip'):\n",
        "  downloaded = drive.CreateFile({'id':file_id})\n",
        "  downloaded.GetContentFile(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2r9UriyQiDP",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "%%capture\n",
        "os.chdir('/content/')\n",
        "!rm data.zip\n",
        "!rm -r sign_classifier\n",
        "gdrive_zip_to_working_dir(GDRIVE_FILE_ID);\n",
        "!unzip data.zip;\n",
        "!rm data.zip\n",
        "os.chdir('sign_classifier')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThgjQtNiRojQ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def print_dir(dir, level=0):\n",
        "  print('|- ' * level, dir.split('/')[-1],sep='')\n",
        "  [print_dir(os.path.join(dir, d), level+1) for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8pesJUEQd0u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "0686d177-275e-423d-c283-bf54b2851c1b"
      },
      "source": [
        "print('Current working directory is:',os.getcwd())\n",
        "print('And the directory looks like this:')\n",
        "print_dir(os.getcwd())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current working directory is: /content/sign_classifier\n",
            "And the directory looks like this:\n",
            "sign_classifier\n",
            "|- augm_images\n",
            "|- data\n",
            "|- |- test\n",
            "|- |- |- Slippery_road\n",
            "|- |- |- Intersection\n",
            "|- |- |- Stop\n",
            "|- |- |- Bikes\n",
            "|- |- |- Forbidden_for_traffic\n",
            "|- |- |- Yield\n",
            "|- |- |- No_entry\n",
            "|- |- |- Right_of_way\n",
            "|- |- |- Pedestrians\n",
            "|- |- |- Speed_60\n",
            "|- |- test_subset\n",
            "|- |- |- Slippery_road\n",
            "|- |- |- Intersection\n",
            "|- |- |- Stop\n",
            "|- |- |- Bikes\n",
            "|- |- |- Forbidden_for_traffic\n",
            "|- |- |- Yield\n",
            "|- |- |- No_entry\n",
            "|- |- |- Right_of_way\n",
            "|- |- |- Pedestrians\n",
            "|- |- |- Speed_60\n",
            "|- |- train\n",
            "|- |- |- Slippery_road\n",
            "|- |- |- Intersection\n",
            "|- |- |- Stop\n",
            "|- |- |- Bikes\n",
            "|- |- |- Forbidden_for_traffic\n",
            "|- |- |- Yield\n",
            "|- |- |- No_entry\n",
            "|- |- |- Right_of_way\n",
            "|- |- |- Pedestrians\n",
            "|- |- |- Speed_60\n",
            "|- |- val\n",
            "|- |- |- Slippery_road\n",
            "|- |- |- Intersection\n",
            "|- |- |- Stop\n",
            "|- |- |- Bikes\n",
            "|- |- |- Forbidden_for_traffic\n",
            "|- |- |- Yield\n",
            "|- |- |- No_entry\n",
            "|- |- |- Right_of_way\n",
            "|- |- |- Pedestrians\n",
            "|- |- |- Speed_60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSyTwft4tqQf",
        "colab_type": "text"
      },
      "source": [
        "## Folder structure for replacing included images with your own data: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXxLZ9SEtqQg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Place your images in subfolders under the main folder \"data/\" with the name of the image category as subfolder name, as in the example folder structure shown below. \n",
        "You need to split the images between \"training data\", \"validation data\" and \"test data\", as the model uses example images from the training folder during the training of the model. The validation data is used as an early indicator of accuracy to optimize model parameters. The test data is then used as the final assesment in the end to test the accuracy of the model on a totally independent set of images. One way of splitting the images between \"train\" \"validate\" and \"test\" is e.g to use 80% of the images for training the model, and validate/test on 10% each.  \n",
        "\n",
        "\n",
        "For a brief introduction to the importance of seperating between \"train\", \"validation\" and \"test\" data, you can have a read here: https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets\n",
        "\n",
        "\n",
        "\n",
        "Example folder structure with included dataset:\n",
        "\n",
        "Training data:\n",
        "\n",
        "- data/train/category_1 : Images of signs from category 1\n",
        "\n",
        "- data/train/category_2 : Images of signs from category 2\n",
        "\n",
        "- .........................................\n",
        "\n",
        "Validation data:\n",
        "\n",
        "- data/val/category_1 : Images of signs from category 1\n",
        "\n",
        "- data/val/category_2 : Images of signs from category 2\n",
        "\n",
        "- .........................................\n",
        "\n",
        "\n",
        "Test data: \n",
        "\n",
        "- data/test/category_1 : Images of signs from category 1\n",
        "\n",
        "- data/test/category_2 : Images of signs from category 2\n",
        "\n",
        "- ........................................."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx2c3G15tqQh",
        "colab_type": "text"
      },
      "source": [
        "## Import various libraries and packages neccesary for defining and running the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX_NZa8ctqQi",
        "colab_type": "text"
      },
      "source": [
        "These are some python libraries/packages that makes our life a lot simpler, as we do not have to write all the code and functionality from scratch. Building a deep learning model from scratch without any of these libraries/packages would actually be a tremendous task! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NCEP-DG0VxlV",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBcODEnztqQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'figure.max_open_warning': 0})\n",
        "import seaborn as sns\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(1337)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(42)\n",
        "\n",
        "from tensorflow.python.keras.applications import vgg16\n",
        "from tensorflow.python.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.python.keras import layers, models, Model, optimizers\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from plot_conf_matr import plot_confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6CO-uBytqQn",
        "colab_type": "text"
      },
      "source": [
        "## Define train/test data and categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIENl-fMtqQn",
        "colab_type": "text"
      },
      "source": [
        "Define folders for the location of train/val/test images and the names of all the different categories we want to classify. \n",
        "We then plot the number of images per category in the training set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JQKkc6DtqQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_dir = \"data/train\"\n",
        "val_data_dir = \"data/val\"\n",
        "test_data_dir = \"data/test\"\n",
        "\n",
        "category_names = sorted(os.listdir('data/train'))\n",
        "nb_categories = len(category_names)\n",
        "img_pr_cat = []\n",
        "\n",
        "for category in category_names:\n",
        "    folder = 'data/train' + '/' + category\n",
        "    img_pr_cat.append(len(os.listdir(folder)))\n",
        "\n",
        "sns.barplot(y=category_names, x=img_pr_cat).set_title(\"Number of training images per category:\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYCWhAGjtqQr",
        "colab_type": "text"
      },
      "source": [
        "As a start, let us also plot an example image from each of the sign categories, to visualize typical image quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz_r6cIbtqQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for subdir, dirs, files in os.walk('data/train'):\n",
        "    for file in files:\n",
        "        img_file = subdir + '/' + file\n",
        "        image = load_img(img_file)\n",
        "        plt.figure()\n",
        "        plt.title(subdir)\n",
        "        plt.imshow(image)\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiRs-4MVtqQv",
        "colab_type": "text"
      },
      "source": [
        "As you can see from the example images, the resolution and quality are not great. However, both image quality and amount of data are often quite limited in practical applications of machine learning. As such, low quality images limited to a maximum of 200 training images per category represents a more realistic example than using \"perfect\" high quality images.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMDapJlZtqQw",
        "colab_type": "text"
      },
      "source": [
        "## Loading a pre-trained Deep Learning model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pik7UYpbtqQx",
        "colab_type": "text"
      },
      "source": [
        " There is no need at this stage to understand the details of the various types of deep learning models, but a summary of some common ways of building models can be found here for those interested (https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751). \n",
        "\n",
        "In this case, we use an already pre-trained deep learning model (VGG16) as the basis for our image classifier model, and then retrain the model on our own data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GE5yTqbtqQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_height, img_width = 224,224\n",
        "conv_base = vgg16.VGG16(weights='imagenet', include_top=False, pooling='max', input_shape = (img_width, img_height, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECEUfRs5tqQ1",
        "colab_type": "text"
      },
      "source": [
        "Having loaded the pre-trained model, we can choose to freeze the \"deeper layers\" of the model in the code block below, and only re-train the last few layers on our own data. This is a common transfer learning stragety, as explained in the article linked above, and is often a good approach when the amount of data available for training is limited.\n",
        "\n",
        "This option is currently commented out from the code (using the #symbol), and we are thus retraining all layers of the model. The number of layers to train represents a parameter you can experiment with yourselves. How does the number of trainable layers affect model performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elPQHrzQtqQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for layer in conv_base.layers[:-13]:\n",
        "#    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHVbnQVqtqQ7",
        "colab_type": "text"
      },
      "source": [
        "As a check we also print a list of all layers of the model, and whether they are trainable or not (True/False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01aLLMFltqQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in conv_base.layers:\n",
        "    print(layer, layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHTSQv5utqQ-",
        "colab_type": "text"
      },
      "source": [
        "Using the VGG16 model as a basis, we build a final classification layer on top to predict our defined classes. We then print a model summary, lisiting the number of parameters of the model. If you decide to \"freeze\" some of the layers, you will notice that the number of \"Trainable parameters\" below will be lower. \n",
        "\n",
        "As you can see, the output shape corresponds to the number of categories, which in our case is 10. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7NnYIGGtqQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Dense(nb_categories, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrZIjl5ltqRB",
        "colab_type": "text"
      },
      "source": [
        "## Define generators that read images from our folders and feeds them to the image classifier models for training/testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZRHz5dctqRC",
        "colab_type": "text"
      },
      "source": [
        "We need to define some functions that read images from our folders and feeds them to the image classifier models. As a part of this we also add some basic image preprocessing, where the input images are scaled to have all pixel values in the range [0,1], (from 0-255 in the original images)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blbe4o0htqRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Number of images to load at each iteration\n",
        "batch_size = 32\n",
        "\n",
        "# only rescaling\n",
        "train_datagen =  ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "test_datagen =  ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "# these are generators for train/test data that will read pictures found in\n",
        "# the defined subfolders of 'data/'\n",
        "\n",
        "print('Total number of images for \"training\":')\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "train_data_dir,\n",
        "target_size = (img_height, img_width),\n",
        "batch_size = batch_size, \n",
        "class_mode = \"categorical\")\n",
        "\n",
        "print('Total number of images for \"validation\":')\n",
        "val_generator = test_datagen.flow_from_directory(\n",
        "val_data_dir,\n",
        "target_size = (img_height, img_width),\n",
        "batch_size = batch_size,\n",
        "class_mode = \"categorical\",\n",
        "shuffle=False)\n",
        "\n",
        "print('Total number of images for \"testing\":')\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "test_data_dir,\n",
        "target_size = (img_height, img_width),\n",
        "batch_size = batch_size,\n",
        "class_mode = \"categorical\",\n",
        "shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JK8w5catqRG",
        "colab_type": "text"
      },
      "source": [
        "## Define final model parameters and start training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b-LUPMytqRH",
        "colab_type": "text"
      },
      "source": [
        "Here, we define some parameters that controls the training process of the model. Important parameters are e.g. training rate, how many epochs to train the model and which optimizer to use. You do not need to understand all these terms to follow the tutorial, but those interested can have a quick read here: https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9\n",
        "\n",
        "We also define a checkpoint parameter, where we keep track of the validation accuracy after each epoch during training. Using this, we save the model that performs best during the training process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtwImtLKtqRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 5e-5\n",
        "epochs = 10\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"sign_classifier.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=learning_rate, clipnorm = 1.), metrics = ['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHy-_BkPtqRL",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to start training the model on our own data: For each \"epoch\" we print out some relevant information from the training process. Importantly, we want the accuracy of our model to be as good as possible. The model accuracy, as measured on the training data, is given by \"acc\", and the accuracy on the images in the test set is given by \"val_acc\". The \"val_acc\" is the most important quantity, as this tells us how accurate the model is on images it has not already seen during the training process. \n",
        "\n",
        "Ideally the \"val_acc\" should increase as we keep training the model, and will eventually reach a steady value when our model is not able to learn any more useful information from our training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTFtV0SptqRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(train_generator, \n",
        "                              epochs=epochs, \n",
        "                              shuffle=True, \n",
        "                              validation_data=val_generator,\n",
        "                              callbacks=[checkpoint]\n",
        "                              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tARSXjfJtqRQ",
        "colab_type": "text"
      },
      "source": [
        "After the training has completed, we load the checkpoint file which had the best validation accuracy during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "175olsCvtqRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.load_model(\"sign_classifier.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOwsIzQ2tqRT",
        "colab_type": "text"
      },
      "source": [
        "## Model accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL0KQBpztqRU",
        "colab_type": "text"
      },
      "source": [
        "To evaluate the model accuracy, we can plot how the model performance changes during the training process. This gives us important information to evaluate what we can do to improve model performance. For a nice introduction to this topic, you can also have a look at this video: https://www.youtube.com/watch?v=yr_qzEzhwqM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiPlePmHtqRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1,len(acc)+1)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, acc, 'b', label = 'Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "#plt.savefig('Accuracy.jpg')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'b', label = 'Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "#plt.yscale('log')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "#plt.savefig('Loss.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er37bWHRtqRY",
        "colab_type": "text"
      },
      "source": [
        "What we can interpret from these curves is as follows: \n",
        "\n",
        "Starting with the upper figure of training/valication accuracy: \n",
        "\n",
        "The blue line represents the model accuracy as measured on the training images, and we see that this quickly reaches a value of almost 1 (which represents classifying 100% of the training images correctly). However, the validation accuracy is the accuracy measured on the validation/test set, which is the accuracy we really care about. In this case, the accuracy leveled off at around 97-98%, meaning that we succesfully clasified almost all of the images in our test set to the correct category. \n",
        "\n",
        "To learn a bit more about the accuracy for each of the categories, we can calculate and plot the \"confusion matrix\", which is an easy way of visualizing the model performance (https://en.wikipedia.org/wiki/Confusion_matrix). The confusion matrix is calculated and plotted through a function called \"plot_confusion_matrix\" in the included script \"plot_conf.py\", for those interested in having a look at the code. This matrix compares the \"true\" vs. \"predicted\" class for all images in the test set. \n",
        "\n",
        "Note: do not worry if you do not get exactly the same numbers when re-running the code! There are some inherent randomness in model initialization etc. which make the results differ slightly from time to time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQSXmqiDtqRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred = model.predict_generator(test_generator)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "cm = confusion_matrix(test_generator.classes, y_pred)\n",
        "plot_confusion_matrix(cm, classes = category_names, title='Confusion Matrix', normalize=False, figname = 'Confusion_matrix_concrete.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pslaLjJCtqRc",
        "colab_type": "text"
      },
      "source": [
        "As seen from the confusion matrix above, the main category the model misclassifies is \"Intersection\", where it mistakes the category with that of \"Yield\" in 10 of the images.  \n",
        "\n",
        "We can also calculate the accuracy averaged over all the different classes: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4PRcrXytqRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = accuracy_score(test_generator.classes, y_pred)\n",
        "print(\"Accuracy in test set: %0.1f%% \" % (accuracy * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X_TGJyCtqRh",
        "colab_type": "text"
      },
      "source": [
        "# Model with image augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLc1qZKhtqRi",
        "colab_type": "text"
      },
      "source": [
        "As we have limited training data, there are some tricks we could try to improve that. In our case, the model already performs very well with an accuracy of 97-98% and it is not certain we are able to improve that further. However, one strategy when dealing with limited training data is that of \"image augmentation\". That is, we make a collection of copies of the existing images but with some minor changes. Those changes could be transformations like e.g. rotation images slightly, zooming, flipping images horizontally ++). \n",
        "Examples also covered here: https://towardsdatascience.com/image-augmentation-for-deep-learning-histogram-equalization-a71387f609b2\n",
        "\n",
        "In the following, we define the same model as above, but we here also incorporate image augmentation as a way of artficially increasing the amount of training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uard9uptqRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base = vgg16.VGG16(weights='imagenet', include_top=False, pooling='max', input_shape = (img_width, img_height, 3))\n",
        "\n",
        "#for layer in conv_base.layers[:-13]:\n",
        "#    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhT_4YtytqRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Dense(nb_categories, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJPW1o5_tqRn",
        "colab_type": "text"
      },
      "source": [
        "## Augmentations: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdEL9bSVtqRo",
        "colab_type": "text"
      },
      "source": [
        "The only thing we need to change in our code, is the function \"training datagen\". We can here define some data augmentation stragtegies, such as random rotation in the range [-10,10] degrees, a random zoom and width/height shift in the range +-10%, and changes in brightness in the range +-10%. \n",
        "\n",
        "As an example of augmented images, we save them to a specified folder \"augm_images\" as defined in the function \"train_generator\" below. This option is currently commented out using the #symbol in the code block, to avoid saving thousands of images, but some previous examples of augmented images are included in the folder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd_ZCpD7tqRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=10,\n",
        "        zoom_range=0.1,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        horizontal_flip=False,\n",
        "        brightness_range = (0.9,1.1),\n",
        "        fill_mode='nearest'\n",
        "        )\n",
        "\n",
        "# this is a generator that will read pictures found in\n",
        "# subfolers of 'data/train', and indefinitely generate\n",
        "# batches of augmented image data\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "train_data_dir,\n",
        "target_size = (img_height, img_width),\n",
        "batch_size = batch_size, \n",
        "#save_to_dir='augm_images', \n",
        "save_prefix='aug', \n",
        "save_format='jpg',\n",
        "class_mode = \"categorical\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suk9AS2htqRr",
        "colab_type": "text"
      },
      "source": [
        "Some examples of augmented images: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM6cIOSStqRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_nr = 1\n",
        "for subdir, dirs, files in os.walk('augm_images'):\n",
        "    for file in files:\n",
        "        img_file = subdir + '/' + file\n",
        "        image = load_img(img_file,target_size=(img_height,img_width))\n",
        "        plt.figure()\n",
        "        plt.title('Augmented image nr: ' + str(img_nr))\n",
        "        plt.imshow(image)\n",
        "        img_nr = img_nr +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvhQe9matqRu",
        "colab_type": "text"
      },
      "source": [
        "## Train new model using augmented data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPq3UwljtqRv",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to train the same model using additional augmented data, which should hopefully increase model performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb13u0h2tqRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 5e-5\n",
        "epochs = 20\n",
        "checkpoint = ModelCheckpoint(\"sign_classifier_augm.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=learning_rate, clipnorm=1.), metrics = ['acc'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIPt5qEQtqRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(train_generator, \n",
        "                              epochs=epochs, \n",
        "                              shuffle=True, \n",
        "                              validation_data=test_generator,\n",
        "                              callbacks=[checkpoint]\n",
        "                              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALvGCzdwtqR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.load_model(\"sign_classifier_augm.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkRM-wYetqR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1,len(acc)+1)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, acc, 'b', label = 'Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "#plt.savefig('Accuracy_Augmented.jpg')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'b', label = 'Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "#plt.savefig('Loss_Augmented.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUbusmsxtqR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred = model.predict_generator(test_generator)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "cm_aug = confusion_matrix(test_generator.classes, y_pred)\n",
        "plot_confusion_matrix(cm_aug, classes = category_names, title='Confusion Matrix', normalize=False, figname = 'Confusion_matrix_Augm.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk2q_jP7tqR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = accuracy_score(test_generator.classes, y_pred)\n",
        "print(\"Accuracy in test set: %0.1f%% \" % (accuracy * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-Mw6i0FtqR_",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation of model accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLj5Ab91tqSA",
        "colab_type": "text"
      },
      "source": [
        "As seen from the above results for model accuracy, data augmentation indeed increased the accuracy of our model. In the current example, we experienced optained a final accuracy of approximately 99%. In addition to the total accuracy, by inspecting the confusion matrix above we can chech which of the sign categories the model classifies incorrectly. Here, we notice that the model stil misclassifies \"Intersection\" as \"Yield\" in a few cases, but significantly better than the model without image augmentation. \n",
        "\n",
        "Note: do not worry if you do not get exactly the same numbers when re-running the code! There is some inherent randomness in the model initialization etc. which could make the results differ slightly from time to time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvOTBTrgtqSA",
        "colab_type": "text"
      },
      "source": [
        "## Plot a few images from the test set, and compare model prediction with ground truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s9mbGm1tqSB",
        "colab_type": "text"
      },
      "source": [
        "As a final visualization of model accuracy, we can plot a subset of the test images along with the corresponding model prediction. Do you agree with the classifications?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fPEiw75tqSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_subset_data_dir = \"data/test_subset\"\n",
        "\n",
        "test_subset_generator = test_datagen.flow_from_directory(\n",
        "test_subset_data_dir,\n",
        "batch_size = batch_size,\n",
        "target_size = (img_height, img_width),\n",
        "class_mode = \"categorical\",\n",
        "shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS76tJZvtqSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred = model.predict_generator(test_subset_generator)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "img_nr = 0\n",
        "for subdir, dirs, files in os.walk('data/test_subset'):\n",
        "    for file in files:\n",
        "        img_file = subdir + '/' + file\n",
        "        image = load_img(img_file,target_size=(img_height,img_width))\n",
        "        pred_emotion = category_names[y_pred[img_nr]]\n",
        "        real_emotion = category_names[test_subset_generator.classes[img_nr]]\n",
        "        plt.figure()\n",
        "        plt.title('Predicted: ' + pred_emotion + '\\n' + 'Actual:      ' + real_emotion)\n",
        "        plt.imshow(image)\n",
        "        img_nr = img_nr +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXEbYqrGtqSJ",
        "colab_type": "text"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc-s-wNYtqSJ",
        "colab_type": "text"
      },
      "source": [
        "If you managed to run through the entire tutorial using the included dataset, you have hopefully gotten a feeling for how deep learning and image recognition can be used to solve a real-world problem of traffic sign classification. \n",
        "\n",
        "Can you think of any interesting use cases where classifying images into different categories can be of interest? Feel free to explore the model with either data from your company, or with images found from e.g. google image search.\n",
        "\n",
        "How many images do you need to reach a sufficient accuracy? And does it help to implement data augmentation? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BaAA5EyZJMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}